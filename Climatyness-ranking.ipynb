{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import utils\n",
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.pipeline import Sentencizer\n",
    "from scipy.spatial import distance\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(cc_list):\n",
    "    '''\n",
    "    Cleans the list of climate change words.\n",
    "    '''\n",
    "    cc_list = [w.replace(\"\\n\", \"\") for w in cc_list]\n",
    "    cc_list = [w.replace(\" \", \"-\").lower() for w in cc_list]\n",
    "    return cc_list\n",
    "\n",
    "def load_cc_words(type=\"normal\"):\n",
    "    '''\n",
    "    Loads the glosssaries of climate change words.\n",
    "    '''\n",
    "    if type == \"normal\":\n",
    "        cc_words = open(\"data/resources/CCglossaryWiki.txt\", \"r\").readlines()\n",
    "        cc_words = clean(cc_words)\n",
    "        return cc_words\n",
    "    elif type == \"enriched\":\n",
    "        cc_words = pd.read_csv(\"data/resources/CCglossaryComplete.csv\", index_col=False, names=['word'])['word'].values\n",
    "        cc_words = clean(cc_words)\n",
    "        return cc_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = utils.load_corpus()\n",
    "cc_words = load_cc_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy stuff\n",
    "nlp = English()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Initialize GloVe embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "glove_embeddings = gensim.downloader.load(\"glove-wiki-gigaword-300\")\n",
    "glove_embeddings.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to assign climate scores to articles? \n",
    "\n",
    "One way is to calculate cosine distance between the climate change topic vector and every sentence in an article, summing these scores, and averaging them. Since cosine distance gives a score between 0 and 1, averaging over the sum of scores gives a rating for the climatyness of an article. \n",
    "\n",
    "Another way is to calculate climateness distributions rather than a single score for each article. We can calculate scores every sentence and assign them into bins (e.g. 0-20, 21-40, 41-60...).\n",
    "\n",
    "Also check out these discussions:\n",
    "- should word vectors be normalized? https://stackoverflow.com/questions/41387000/cosine-similarity-of-word2vec-more-than-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1784161637422509\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Example of cosine distance\n",
    "a = [2, 0, 1, 1, 0, 2, 1, 1]\n",
    "b = [2, 1, 1, 0, 1, 1, 1, 1]\n",
    "\n",
    "print(distance.cosine(a,b)) \n",
    "print(distance.cosine(a,a)) # 0.0 means perfect similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangle the data so it can be used easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "journals = [\"ScienceOCR\", \"NatureOCR\"]\n",
    "for journal in journals:\n",
    "    for article in corpus[journal]:\n",
    "        text = article[1]\n",
    "        sentences = []\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            if sent.orth_ != \"\\n\":\n",
    "                s = sent.orth_.replace(\"\\n\", \"\")\n",
    "                sentences.append(s)\n",
    "        articles.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for article in articles:\n",
    "    tokenized_sents = []\n",
    "    for sent in article:\n",
    "        tokenized_sents.append([token.orth_.lower() for token in tokenizer(sent) if token.orth_.lower() in glove_embeddings.vocab and token.orth_.lower() not in stop_words])\n",
    "    tokenized.append(tokenized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the average vector of a sentence\n",
    "def get_vec(tokenized_sentence):\n",
    "    return np.mean(np.array([glove_embeddings[word] for word in tokenized_sentence]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the list of cosine distances and prints the article according to this ranking,\n",
    "# from most climaty to least climaty\n",
    "def print_article(article, scores):\n",
    "    sorted_scores = sorted(list(enumerate(scores)), key=lambda x:x[1])\n",
    "    sorted_indices = [el[0] for el in sorted_scores]\n",
    "    print(\"Printing article from most to least climaty sentences:\\n\")\n",
    "    for i in range(len(scores)):\n",
    "        try:\n",
    "            print(\"{}\\n\".format(article[sorted_indices[i]]))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: using keywords from Hulme et al. paper.\n",
    "\n",
    "Hulme et al. created the corpus by combining the articles containing one or more mentions of the following words: ‘climate’, ‘greenhouse’, ‘carbon’, ‘warming’, ‘weather’, ‘atmosphere’, ‘pollution’. We'll use these keywords to create a generic vector representing the topic of climate change. We'll then use this vector to calculate how likely the article is to be about climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_keywords = [\"climate\", \"greenhouse\", \"carbon\", \"warming\", \"weather\", \"atmosphere\", \"pollution\"]\n",
    "cc_embedding = get_vec(cc_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we calculate similarity scores for every sentence in every article. `similarity_scores` is a list of lists — its length is 493, one list for each article. The elements of each sublist are cosine distance scores between a sentence and the climate change embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = []\n",
    "for article in tokenized:\n",
    "    current_article = []\n",
    "    for tokenized_sentence in article:\n",
    "        sentence_embedding = get_vec(tokenized_sentence)\n",
    "        score = distance.cosine(cc_embedding, sentence_embedding)\n",
    "        current_article.append(score)\n",
    "    similarity_scores.append(current_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "# make sure that the number of elements corresponds to the number of articles\n",
    "print(len(similarity_scores) == 493)\n",
    "\n",
    "# make sure that the number of scores for the first article corresponds to the number of sentences\n",
    "print(len(similarity_scores[0]) == len(articles[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing article from most to least climaty sentences:\n",
      "\n",
      "One of several ways to attenuate the increase of CO2 in the atmosphere is to sequester it.*\n",
      "\n",
      "Worldwide emissions of CO2 continue to increase, and prudence dictates that technologies be developed to help limit this trend.\n",
      "\n",
      "However, if international agreements are implemented to attenuate the buildup of atmospheric CO2, sequestration of it in unused or abandoned fossil hydrocarbon fields is one good step to take.\n",
      "\n",
      "A vigorous program aimed at cutting the cost of cleaning CO2 emitted by power plants should have a high priority and adequate funding.\n",
      "\n",
      "The injection of CO2 into oil fields is having economically beneficial effects while at the same time sequestering CO2.\n",
      "\n",
      " A major source of CO2 is the combustion of fossil fuel in power plants.\n",
      "\n",
      "Controversy exists about the possible extent of the contribution of CO2 to present and future global warming.\n",
      "\n",
      " The present amount of geological sequestration of CO2 is small in comparison with emissions.\n",
      "\n",
      "In addition to its use in petroleum fields, a substantial volume of CO2 could be sequestered in deep unmineable coal and in depleted natural gas fields.\n",
      "\n",
      "The IEA report estimates that the ultimate storage capacity of the oil and gas fields equates to over 125 years of total current CO2 emissions from fossil-fueled power plants.\n",
      "\n",
      "Coal is the major and the cheapest fuel, but CO2 formed by burning it is polluted.\n",
      "\n",
      " Almost all of the efforts to use CO2 in enhanced oil recovery have occurred in the United States.\n",
      "\n",
      "The net result can be profitable use of CO2 and sequestration of some of it.\n",
      "\n",
      "Injection of CO2 was successful in increasing production.\n",
      "\n",
      "In addition, under the surface of the earth in the United States and elsewhere, many structures that once were filled with fossil fuels have been exploited, leaving space that might be used to store CO2.\n",
      "\n",
      "Much of the CO2 is recycled, but part of it remains in the oil-bearing formations.\n",
      "\n",
      "In many oil wells, pressures and temperatures are high enough that CO2 becomes a mobile fluid that is miscible with oils having densities of about 0.90 grams per cubic centimeter or less.\n",
      "\n",
      "In the United States in 1998, CO2 had a major role in producing 6% of domestic crude oil.\n",
      "\n",
      "Success in such a program would be an important help in reducing the overall costs of whatever sequestration methods were ultimately employed.\n",
      "\n",
      "Later it was discovered that CO2 under pressure was also effective.\n",
      "\n",
      "In the United States, some of the structures from which natural gas has been extracted are being used to safely store high-pressure gas produced elsewhere.\n",
      "\n",
      "Total miscibility was not achieved, but a large volume of CO2 dissolved in the oil and its viscosity decreased from 1000 centiposes (cp) to less than 100 cp.\n",
      "\n",
      "These liquids tend to move toward lower pressures in production wells, and with this technique an additional 10 to 15% of the oil in an oil field can often be produced.\n",
      "\n",
      "However, the ultimate global potential is substantial.\n",
      "\n",
      " Later this year, a project using CO2 will begin at Weymouth Sasketchawan in Canada.\n",
      "\n",
      "Its heavy oil has high-molecular-weight components, and primary production was capable of achieving an ultimate recovery of only 1.5% of the original oil.\n",
      "\n",
      "A field there that has been producing oil since 1952 will receive 5000 metric tons of CO2 per day via a 300-kilometer pipeline.\n",
      "\n",
      "In the early days, production was often enhanced in neighboring wells when water was forced down into one of them.\n",
      "\n",
      "The gas must be cleaned and pressurized before injection.\n",
      "\n",
      "A 131-page report commissioned by the International Energy Agency (IEA) and supported in part by the U.S. Department of Energy has included estimates of what might ultimately be achieved. †\n",
      "\n",
      "During the course of a multiyear injection, 20 million metric tons of CO2 will be sequestered, and an extra 130 million barrels of oil will be produced.\n",
      "\n",
      "Another benefit could be that more attention will be focused on the Dakota Gasification Company.\n",
      "\n",
      " In the exploitation of petroleum fields, about 20 to 40% of the oil can usually be obtained.\n",
      "\n",
      "An international team will monitor the behavior of the field, and details of their findings will be made widely available.\n",
      "\n",
      "The technology for doing so exists.\n",
      "\n",
      "The total cost of these procedures is roughly $53 per metric ton or more.\n",
      "\n",
      "This cost is a barrier to segregating it.\n",
      "\n",
      "Its principal inputs are liquid oxygen, steam, and lignite coal.\n",
      "\n",
      "This facilitated flow, and a recovery of 6.5% is expected.\n",
      "\n",
      "Among the plant's other outputs are electricity, methane, hydrogen, ammonium sulfate, phenols, and cresols.\n",
      "\n",
      "A notable exception is a large Bati Roman field in southeast Turkey.\n",
      "\n",
      "The source will be the Dakota Gasification Company, located in Beulah, North Dakota.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print article 0\n",
    "print_article(articles[0], similarity_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3401578664779663, 0.7683388441801071)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(similarity_scores[0]), max(similarity_scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: using Wikipedia's glossary of climate change terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the glossary so it only contains words that are in the model's vocabulary\n",
    "filtered = [w for w in cc_words if w in glove_embeddings.vocab]\n",
    "print(\"Number of words prior to filtering: ({}) and after filtering: ({})\".format(len(cc_words), len(filtered)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
